---
title: Linee guida per l’introduzione dell’AI a scuola
description: Analisi delle Linee guida per l’introduzione dell’Intelligenza Artificiale nelle Istituzioni scolastiche Versione 1.0 - Anno 2025 .
---

## Linee guida MIM per l’introduzione dell’Intelligenza Artificiale nelle Istituzioni scolastiche (v.1.0, 2025): un’analisi per le scuole secondarie di secondo grado

*Questa pagina riassume e interpreta i contenuti del documento ministeriale [“Linee guida per l’introduzione dell’Intelligenza Artificiale nelle Istituzioni scolastiche – Versione 1.0, 2025”](https://www.mim.gov.it/documents/20182/0/MIM_Linee+guida+IA+nella+Scuola_09_08_2025-signed.pdf/b70fdc45-4b75-1f7e-73bf-eab12989b928?t=1756468797694), con rinvii puntuali ai relativi paragrafi.*[^1]

---

## Abstract

Le **Linee guida del Ministero dell’istruzione e del merito (MIM)** per l’introduzione dell’Intelligenza Artificiale (IA) nella scuola delineano un quadro di **principi, requisiti e processi operativi** finalizzato a promuovere un’adozione **antropocentrica, equa, sicura e conforme** alla normativa vigente. Il documento definisce, da un lato, **obiettivi strategici** (miglioramento degli apprendimenti, inclusione, efficienza amministrativa, qualità dei servizi) e, dall’altro, **obblighi e cautele** connesse ai diversi livelli di rischio dei sistemi di IA, in particolare quando impattano su **accesso, valutazione e monitoraggio** in ambito educativo. Il modello proposto – basato su **quattro pilastri** (principi, requisiti, framework di implementazione, comunicazione e governance) – si concretizza in un **ciclo in cinque fasi** (definizione, pianificazione, adozione, monitoraggio, conclusione), accompagnato da **focus applicativi** per i principali destinatari (dirigenti, personale amministrativo, docenti, studenti) e da **misure di mitigazione** dei rischi. Per le scuole secondarie di secondo grado, ciò implica una **regia pedagogica e organizzativa** capace di integrare le opportunità dell’IA con il rispetto dei **diritti fondamentali, della protezione dei dati e della libertà di insegnamento**, valorizzando la responsabilità professionale dei docenti e l’alfabetizzazione all’IA dell’intera comunità scolastica.[^2],[^3]

---

## 1. Contesto e cornice regolatoria

Le Linee guida nascono all’incrocio di **indirizzi internazionali, europei e nazionali**, in una fase in cui gli impieghi educativi dell’IA accelerano, ma richiedono **governo pubblico**, **trasparenza** e **garanzie**.[^4] Il testo richiama, tra l’altro:

- il **Regolamento europeo sull’Intelligenza Artificiale (AI Act)**, con i relativi **considerando** e gli obblighi per fornitori e **deployer** (le istituzioni scolastiche utilizzatrici), con particolare attenzione ai **sistemi ad alto rischio** in ambito educativo (accesso/ammissione, assegnazione, valutazione degli apprendimenti, monitoraggio dei comportamenti durante le prove);[^5][^6]
- la **Raccomandazione UNESCO sull’etica dell’IA** (2021) e gli **Orientamenti etici della Commissione europea** sull’uso dell’IA e dei dati nell’insegnamento e apprendimento per i docenti;[^7][^8]
- la **Strategia italiana per l’IA 2024–2026** e il **disegno di legge nazionale** in materia di IA;[^9]
- il **GDPR** e la disciplina italiana di protezione dei dati personali (con cenni agli atti e ai provvedimenti del Garante), in particolare per i minori, considerati **interessati vulnerabili**.[^10]

In questo contesto, il Ministero fissa **principi, requisiti e strumenti** per un’adozione **strutturata, controllata e rendicontabile** dell’IA in ambito scolastico, con **monitoraggio centrale** tramite la piattaforma **Unica**.[^11]

---

## 2. Strategia e obiettivi: un’IA al servizio della qualità educativa

La strategia del MIM mira a **governare l’introduzione dell’IA** come leva per la **competitività del sistema educativo** e la **qualità dell’offerta formativa**, promuovendo **equità** e **consapevolezza**.[^12] Gli obiettivi operativi dichiarati includono:

1. **Conformità normativa** e metodologia condivisa per le scuole autonome (D.P.R. 275/1999);  
2. **Innovazione antropocentrica, sicura, affidabile, etica**;  
3. **Uso uniforme** e **valori europei** a presidio dei diritti e delle libertà;  
4. **Alfabetizzazione ai rischi** e alle opportunità dell’IA per l’intera comunità scolastica.[^13]

Tra le finalità concrete, si sottolineano: **personalizzazione degli apprendimenti**, **inclusione**, **ottimizzazione dei processi interni**, **miglioramento dei servizi a studenti e famiglie**, **formazione continua** del personale.[^14] La realizzazione di tali finalità è condizionata al **rispetto delle pratiche vietate** dall’AI Act (ad esempio, **emotion recognition** in contesti scolastici, salvo motivi medici o di sicurezza) e all’adesione a **profili di rischio** appropriati.[^15][^16]

---

## 3. Il modello di introduzione: principi, requisiti, framework, governance

Le Linee guida strutturano l’azione in **quattro pilastri** interdipendenti:[^17]

- **Principi di riferimento**: centralità della persona, **equità**, **innovazione responsabile**, **sostenibilità**, **tutela dei diritti**, **sicurezza dei sistemi**;  
- **Requisiti di base**: profili **etici**, **tecnici** e **normativi**;  
- **Framework di implementazione**: modello operativo per l’intero ciclo di vita dei progetti;  
- **Comunicazione e governance**: allineamento tra scuole e Ministero, mappatura sperimentazioni, **servizio digitale Unica**.

### 3.1 Principi di riferimento

L’adozione dell’IA deve **preservare il ruolo dell’umano**, promuovere **pari opportunità**, **inclusione**, **trasparenza** e **explainability**, garantire **privacy by design e by default**, e assicurare **sicurezza** e **resilienza** dei sistemi. La **sostenibilità** è intesa nelle sue tre dimensioni (sociale, economica, ambientale).[^18]

### 3.2 Requisiti etici

- **Intervento e sorveglianza umana**: presenza di **controllo umano significativo** in tutte le fasi, specialmente quando l’IA incide su **opportunità educative** o **processi decisionali** che riguardano gli studenti;[^19]
- **Trasparenza e explainability**: documentazione e **comprensibilità** del funzionamento dei sistemi; attenzione alle **allucinazioni** dei modelli generativi e alla **verifica delle fonti**;[^20]
- **Equità e non discriminazione**: prevenzione dei **bias** nei dataset e nei modelli, monitoraggio e correzione sistematica delle distorsioni;[^21]
- **Ruoli e responsabilità**: chiara attribuzione di compiti (dirigente, personale, stakeholder), **accordi con i fornitori** e presidi su **trattamenti di dati** eventualmente da questi eseguiti come **responsabili**.[^22]

### 3.3 Requisiti tecnici

- **Certificazioni e conformità** dei fornitori (es. ISO/IEC 27001; qualificazioni AgID per SaaS);  
- **Gestione responsabile dei dati**: minimizzazione, ambienti controllati, **assenza di profilazione** non necessaria, **disattivazione di funzioni** non indispensabili (cronologie, servizi accessori), **Data Privacy Framework**;  
- **Diritto di non partecipazione** all’addestramento con dati personali;  
- **Equità algoritmica** e **trasparenza** degli output.[^23]

### 3.4 Requisiti normativi (protezione dei dati)

Il documento ribadisce la **piena applicazione del GDPR** lungo l’intero ciclo di vita dei sistemi di IA: **liceità**, **limitazione delle finalità**, **minimizzazione**, **esattezza**, **limitazione della conservazione**, **integrità e riservatezza**, con specifiche cautele per gli **studenti minorenni**.[^24] Sono evidenziati, tra gli adempimenti a carico del **Titolare del trattamento** (scuola):

- **Base giuridica** chiara e conoscibile;  
- **DPIA** obbligatoria in caso di nuove tecnologie e trattamenti potenzialmente rischiosi; **integrazione con FRIA** (valutazione d’impatto sui diritti fondamentali) per i sistemi **ad alto rischio**;  
- **Informative** trasparenti (artt. 13–14 GDPR) con linguaggio **comprensibile ai minori**;  
- **Nomina e istruzioni** ai soggetti autorizzati e **ai responsabili** (art. 28 GDPR);  
- **Misure tecnico-organizzative** adeguate (art. 32 GDPR), **privacy by design & by default**;  
- **Registro dei trattamenti** (art. 30 GDPR) e **data breach notification** (artt. 33–34 GDPR).[^25]

Una particolare attenzione è rivolta a **age-gate** e presidi per l’accesso ai servizi, specie **infra-tredicenni**, nonché alla gestione di **dati sintetici** e **Privacy-Enhancing Technologies (PET)** per ridurre i rischi.[^26]

---

## 4. Classificazione del rischio e obblighi conseguenti

L’AI Act distingue tra **sistemi ad alto rischio**, **a rischio limitato** e **a rischio minimo o nullo**. In ambito scolastico, sono **ad alto rischio** i sistemi destinati a: **determinare accesso o ammissione**, **assegnare a percorsi**, **valutare risultati dell’apprendimento**, **monitorare comportamenti durante le prove**. Tali sistemi impongono **obblighi rafforzati** per fornitori e **deployer** (scuole), tra cui **valutazioni di conformità**, **governance dei dati**, **formazione del personale** e **FRIA**.[^27]

I sistemi a **rischio limitato** comportano **obblighi di trasparenza** verso gli utenti (es. dichiarare l’interazione con un sistema di IA), mentre quelli a **rischio minimo** sono liberi da obblighi, fermo restando il **ricorso a codici di condotta** per promuovere buone pratiche.[^28]

---

## 5. Il framework di implementazione: cinque fasi operative

Le Linee guida propongono un **approccio metodologico** per introdurre l’IA in modo **graduale, controllato e valutabile**, articolato in **cinque fasi**.[^29]

### 5.1 Definizione (progetto e approvazione iniziale)

- **Analisi del contesto** (maturità digitale, infrastrutture, competenze, processi);  
- **Selezione dei casi d’uso** in base a benefici, rischi, fattibilità e impatti;  
- **Individuazione degli stakeholder** interni/esterni e modalità di coinvolgimento;  
- **Allineamento con il PTOF** e approvazione negli organi collegiali.  
Il coinvolgimento **partecipativo** della comunità scolastica è considerato **determinante**.[^30]

### 5.2 Pianificazione (elaborazione di dettaglio)

- **Piano di progetto** (milestone, team, costi, monitoraggio);  
- **Piano dei rischi** con valutazione **qualitativa** e **prioritizzazione** (scala, portata, reversibilità, probabilità), coerente con la metodologia **HUDERIA** del Consiglio d’Europa;  
- **Comunicazione e allineamento** con stakeholder, rappresentanze, reti di scuole.[^31]

### 5.3 Adozione (implementazione)

- **Approccio graduale** (pilota, estensione progressiva);  
- **Piano di comunicazione** per informare e coinvolgere;  
- **Piano di formazione** per personale e, ove pertinente, studenti, con moduli e metodologie diversificate (e‑learning, workshop, laboratori pratici).[^32]

### 5.4 Monitoraggio (verifica e miglioramento continuo)

- **Monitoraggio gestionale** dell’avanzamento;  
- **Monitoraggio tecnico** degli output (anomalie, risultati inattesi, violazioni);  
- **Rivalutazione periodica dei rischi** e **feedback** a fornitori e stakeholder;  
- **Meccanismi interni** di segnalazione e **rendicontazione**.[^33]

### 5.5 Conclusione (valutazione dei risultati)

- **Verifica del raggiungimento dei target**;  
- **Analisi delle lezioni apprese**;  
- **Documentazione e valorizzazione** dei risultati;  
- **Riconoscimento del team** e **diffusione delle buone pratiche** in ottica di riuso.[^34]

---

## 6. Aree di applicazione per ruoli e funzioni nella scuola secondaria

Le Linee guida offrono esempi d’uso **contestualizzati** per i diversi attori scolastici.[^35]

### 6.1 Dirigente scolastico

- **Coerenza strategica e documentale**: supporto nel **monitoraggio** di PTOF, RAV, Piano di Miglioramento, Atto di indirizzo, Programma annuale, Rendicontazione sociale, con evidenza di **scostamenti e incongruenze**;  
- **Formazione**: analisi dei **fabbisogni** e pianificazione delle **iniziative**;  
- **Organizzazione**: supporto alla **formazione classi**, **distribuzione aule** e **orari** nel rispetto delle norme di sicurezza;  
- **Comunicazione**: modulazione del **tone of voice**, personalizzazione dei messaggi per stakeholder differenti (studenti, famiglie, docenti, territorio).[^36]

### 6.2 Personale amministrativo (DSGA e ATA)

- **Front office digitale**: **chatbot** per smistamento richieste (orari, iscrizioni, certificazioni), verifica documentale;  
- **Comunicazioni periodiche**: mailing automatizzato per scadenze ricorrenti;  
- **Gestione beni mobili e inventario**: analisi fabbisogni, ottimizzazione acquisti e sostituzioni.[^37]

### 6.3 Docenti

- **Personalizzazione didattica**: adattamento di **materiali** e **percorsi** ai livelli di competenza e ai ritmi di apprendimento (con integrazione con **PEI** e **PDP** ove presenti);  
- **Strumenti interattivi**: simulazioni, giochi, mappe concettuali, riassunti, quiz;  
- **Organizzazione di visite e attività extracurricolari**: suggerimenti basati su interessi e **feedback** pregressi;  
- **Rubriche di valutazione**: supporto alla **definizione dei descrittori**;  
- **Tutoraggio**: sostegno ad attività cooperative con domande guida e stimoli al **pensiero critico**.  
Il ruolo docente resta **centrale** nel garantire uso **etico e pedagogicamente fondato**, in coerenza con linee nazionali (STEM, Educazione civica).[^38]

### 6.4 Studenti

- **Curiosità e motivazione**: percorsi personalizzati e **suggerimenti** per approfondimenti;  
- **Integrazione multidisciplinare**: scomposizione di problemi complessi e connessioni tra discipline;  
- **Accessibilità**: supporto multilingue, trascrizioni automatiche, ambienti virtuali;  
- **Autonomia**: chatbot di supporto fuori dall’orario scolastico, sviluppo di **problem solving**;  
- **Feedback immediati**: spiegazioni e correzioni, in aggiunta al **feedback** strutturato del docente.  
Resta fermo il **divieto** di strumenti di **sentiment analysis** a fini educativi previsto dall’AI Act.[^39]

---

## 7. Rischi, cautele e mitigazioni: un approccio gerarchico

L’introduzione dell’IA richiede **valutazioni di rischio** e **misure di mitigazione** secondo la gerarchia HUDERIA: **prevenire**, **ridurre**, **ripristinare/compensare**.[^40] Le azioni raccomandate includono:

- **Protezione dei dati**: misure tecniche/organizzative, **privacy by design/default**, limiti chiari a accesso e uso dei dati, minimizzazione e **PET**;  
- **Manutenzione programmata**: aggiornamenti, monitoraggio reti, backup, test di compatibilità, formazione tecnica di base del personale;  
- **Progettazione etica**: **auditing** dei dataset, test di **equità**, monitoraggio continuo, ricorso a **dati sintetici**;  
- **explainability**: tecniche e metriche di interpretabilità, **diritto alla spiegazione** della logica decisionale;  
- **Formazione continua**: competenze per riconoscere e **mitigare bias**, uso responsabile dei **genAI**;  
- **Sistemi ibridi**: automazione delle routine, centralità della **relazione educativa**;  
- **Interazione sociale**: alternanza tra strumenti di IA e attività collaborative in presenza;  
- **Dialogo e trasparenza**: comunicazioni chiare e aggiornate a tutta la comunità scolastica.[^41]

---

## 8. Governance e piattaforma Unica

La **governance** prevede **allineamento costante** tra Ministero, USR e scuole, con **monitoraggio delle iniziative** e **mappatura delle sperimentazioni** tramite **Unica**, che offrirà: fruizione interattiva delle Linee guida, strumenti operativi per la **compliance** (es. checklist privacy), schede progetto per il **censimento** e la **condivisione** delle esperienze, oltre a una **dashboard** ministeriale per il controllo di conformità e **audit a campione**.[^42]

---

## 9. Implicazioni operative per le scuole secondarie di secondo grado

Per gli istituti di scuola superiore, l’adozione consapevole dell’IA richiede un insieme integrato di scelte **pedagogiche**, **organizzative** e **tecnico‑giuridiche**:

### 9.1 Progettazione didattica e curricolare

- Integrare l’IA nel **curricolo digitale** e nelle **metodologie attive**, con attenzione a **STEM** e **Educazione civica** (cittadinanza digitale), salvaguardando **autenticità** e **originalità** degli apprendimenti.[^43]  
- Incorporare **compiti autentici** che sollecitino spiegazione, argomentazione, confronto di fonti e riflessione metacognitiva, per evitare sostituzioni improprie dei processi cognitivi con risposte generative.

### 9.2 Valutazione e feedback

- Utilizzare strumenti di IA **a supporto** (e non in sostituzione) della valutazione formativa, mantenendo **responsabilità e giudizio** professionale al docente;  
- Evitare l’impiego di sistemi che, per finalità e impatto, ricadono nella **categoria ad alto rischio** (valutazione automatizzata, monitoraggio durante prove) se non nel pieno rispetto delle condizioni di legge, con **DPIA+FRIA**, **sorveglianza umana** e **trasparenza**.[^44]

### 9.3 Protezione dei dati e conformità

- Redigere/aggiornare **informative** chiare per studenti e famiglie;  
- Applicare **minimizzazione**, **anonimizzazione**/pseudonimizzazione ove possibile, **limitazione delle finalità**;  
- Configurare le piattaforme in **ambienti controllati** con **cronologie disattivate** e **no training** sui dati della scuola;  
- Eseguire una **DPIA** per i progetti innovativi e, se ad alto rischio, integrare con **FRIA**;  
- Tenere il **registro dei trattamenti** e predisporre procedure **data breach**.[^45]

### 9.4 Formazione e cultura professionale

- Programmare **formazione continua** su IA generativa, **explainability**, **etica** e **privacy** per docenti e personale;  
- Sviluppare **attività di alfabetizzazione all’IA** per studenti, con attenzione a **pensiero critico**, **valutazione delle fonti**, **uso responsabile** e **limiti** dei modelli (allucinazioni, bias, tempi di aggiornamento).[^20],[^46]

### 9.5 Relazione educativa e benessere

- Preservare la **centralità della relazione** e l’**interazione sociale** tra pari e con i docenti;  
- Prevenire **dipendenze strumentali** o **deleghe cognitive** eccessive all’IA, bilanciando attività on‑line e in presenza;  
- Adottare pratiche inclusive per studenti con **BES/DSA** in coerenza con **PEI/PDP**, valutando attentamente i dati trattati e le misure di tutela.[^47]

### 9.6 Scelte tecnologiche e contrattualistica

- Selezionare fornitori con **certificazioni** e **garanzie** dimostrabili;  
- Stipulare **atti ex art. 28 GDPR** per i responsabili del trattamento, con clausole su **luogo del trattamento**, **sub‑processor**, **diritti di audit**, **sicurezza**, **divieto di profilazione** non necessaria, **no training**;  
- Pianificare **manutenzione**, **backup**, **log** e **monitoraggio** degli accessi.[^48]

---

## 10. Verso una “IA scolastica affidabile”: prospettive e condizioni di successo

La riuscita dell’integrazione dell’IA nella scuola superiore dipende da **condizioni abilitanti**:

1. **Leadership educativa** dei dirigenti e **comunità professionali** dei docenti orientate alla ricerca‑azione;  
2. **Progettazione partecipata** con studenti e famiglie, in quadro di **trasparenza**;  
3. **Formazione** continua e **supporto tra pari** (reti di scuole, comunità di pratica);  
4. **Monitoraggio** e **valutazione** con indicatori condivisi (es. equità, impatto sugli apprendimenti, inclusione, benessere digitale);  
5. **Compliance by design**: DPIA/FRIA, misure di sicurezza, gestione dei diritti degli interessati;  
6. **Documentazione e disseminazione** tramite **Unica**, per favorire il **riuso** e il miglioramento incrementale.[^49]

Una **IA scolastica affidabile** è, in definitiva, quella che **eleva la qualità** dell’insegnamento e dell’apprendimento **senza sostituire** la professionalità docente, che **rispetta** i diritti fondamentali e che **riduce** – non aumenta – le disuguaglianze. L’“intelligenza” dei sistemi deve restare **al servizio dell’intelligenza educativa**.

---

## 11. Esempi pratici per docenti della scuola secondaria di secondo grado

La sezione che segue propone **scenari concreti, trasferibili e conformi** alle Linee guida, con particolare attenzione a **privacy**, **sorveglianza umana** e **equità**. Per ciascun esempio sono indicati **obiettivi**, **procedura**, **impostazioni di tutela**, **valutazione** e **riferimenti** ai paragrafi rilevanti del documento ministeriale (tra parentesi i rimandi alle note già presenti).

> **Avvertenza generale**: ove si impieghino piattaforme di IA **esterne** alla scuola, occorre verificare **informative, basi giuridiche, configurazioni privacy**, disattivare la **cronologia** ove non necessaria, **impedire l’uso per training** e **non inserire dati personali identificativi** degli studenti, in conformità a **3.2** e **3.3** (requisiti tecnici e normativi).[^23],[^25],[^45]

### 11.1 Personalizzazione dell’apprendimento (Italiano e Scienze umane)

- **Esempio 1 — Letture differenziate con supporto metacognitivo**
  - **Obiettivo**: offrire tre versioni graduate di un medesimo brano (lessico semplificato, medio, avanzato) con domande metacognitive.  
  - **Procedura**: il docente seleziona un testo di narrativa o saggistica e richiede al sistema di IA **tre parafrasi** controllate, accompagnate da **domande di riflessione** (es. “Che strategia hai usato per comprendere questo paragrafo?”).  
  - **Tutela**: nessun dato personale; usare **account docente** in ambiente controllato; salvare i materiali su repository d’istituto.  
  - **Valutazione**: rubrica su **comprensione**, **uso di strategie** e **autovalutazione**; feedback orale del docente.  
  - **Riferimenti**: personalizzazione e strumenti interattivi (4.2 Docenti), trasparenza e sorveglianza umana (3.1), privacy by design (3.3).[^38],[^19],[^25]

- **Esempio 2 — Schede “ponte” per studenti NAI**  
  - **Obiettivo**: costruire schede bilingui con glossari visuali per studenti neoarrivati in Italia.  
  - **Procedura**: il docente fornisce parole‑chiave disciplinari; l’IA genera **definizioni semplificate** e **esempi illustrati**; verifica docente; stampa o LMS.  
  - **Tutela**: non inserire nomi/biografie; preferire **immagini generate** senza dati reali; controllare bias culturali.  
  - **Valutazione**: checklist di **acquisizione lessicale** e **uso contestuale**.  
  - **Riferimenti**: equità e inclusione (2), criteri anti‑discriminazione (3.1), gestione responsabile dei dati (3.2).[^18],[^21],[^23]

### 11.2 STEM: Matematica e Fisica

- **Esempio 3 — Generatore di varianti isomorfe di problemi**  
  - **Obiettivo**: fornire serie di esercizi “equivalenti” con **passaggi risolutivi spiegati**.  
  - **Procedura**: il docente inserisce un problema tipo; l’IA produce **5 varianti** modificando numeri e contesto; il docente verifica la **correttezza** e pubblica su LMS.  
  - **Tutela**: nessun dato personale; indicare agli studenti che **non** devono incollare soluzioni su servizi esterni; usare l’istanza scolastica.  
  - **Valutazione**: consegna di **taccuino di calcolo** con ragionamento; valutazione del **processo**, non solo del risultato.  
  - **Riferimenti**: sorveglianza umana e explainability (3.1), strumenti per esercitazione (4.2 Docenti).[^19],[^38]

- **Esempio 4 — Laboratorio virtuale di Fisica (simulazioni)**  
  - **Obiettivo**: indagare relazioni (p. es. moto uniformemente accelerato) con **parametri controllati**.  
  - **Procedura**: uso di ambienti con IA per **guidare ipotesi**, esecuzione di simulazioni, **registrazione dati**, confronto con modelli.  
  - **Tutela**: profili **anonimizzati** su piattaforme esterne; consenso informativo generico per attività digitali.  
  - **Valutazione**: report con **grafici**, **discussione errori** e riflessione su **limitazioni** della simulazione.  
  - **Riferimenti**: innovazione responsabile (2), monitoraggio output e anomalie (4.1 Monitoraggio).[^18],[^33]

### 11.3 Scienze naturali e salute

- **Esempio 5 — “Spiega come a un compagno”: sintesi e domande Socratiche**
  - **Obiettivo**: consolidare concetti complessi (es. trascrizione/traduzione del DNA) con **spiegazioni progressive**.  
  - **Procedura**: l’IA genera spiegazioni a **tre livelli** (base/intermedio/avanzato) e **domande socratiche**; il docente seleziona e integra.  
  - **Tutela**: nessun dato personale; conservazione su LMS.  
  - **Valutazione**: breve colloquio su **consapevolezza degli errori** e **limiti delle spiegazioni**.  
  - **Riferimenti**: trasparenza, allucinazioni e controllo umano (3.1), didattica attiva (4.2 Docenti).[^20],[^38]

- **Esempio 6 — Analisi critica di immagini scientifiche**  
  - **Obiettivo**: distinguere **immagini microscopiche reali** da **immagini sintetiche**.  
  - **Procedura**: il docente prepara un set misto; l’IA e gli studenti **etichettano** e argomentano; confronto con **fonti**.  
  - **Tutela**: immagini prive di metadati sensibili; citazioni corrette.  
  - **Valutazione**: rubrica sulla **qualità dell’argomentazione** e **uso di evidenze**.  
  - **Riferimenti**: cittadinanza digitale e valutazione delle fonti (4.2 Studenti), **DigComp** richiamato (4.2).[^39]

### 11.4 Lingue straniere e CLIL

- **Esempio 7 — Feedback scritto multilivello**  
  - **Obiettivo**: migliorare **coerenza, coesione e registro** nei testi in L2.  
  - **Procedura**: l’IA propone **tre revisioni**: (a) **correzione micro** (ortografia/grammatica), (b) **coerenza** e **connettivi**, (c) **registro**; il docente approva o modifica.  
  - **Tutela**: elaborati **pseudonimizzati**; no caricamenti su servizi pubblici.  
  - **Valutazione**: confronto **bozza–revisioni**; autovalutazione guidata.  
  - **Riferimenti**: feedback immediato e ruolo non sostitutivo (4.2 Studenti; 3.1).[^39],[^19]

- **Esempio 8 — CLIL con scaffolding generativo**  
  - **Obiettivo**: supportare l’apprendimento di contenuti non linguistici in L2 con **materiali ponte** (glossari, organizer grafici).  
  - **Procedura**: il docente chiede all’IA **outline**, **glossari** con immagini, **quiz**; verifica e adatta al livello.  
  - **Tutela**: evitare dati identificativi; repository d’istituto.  
  - **Valutazione**: verifica contenutistica e linguistica con **rubriche**.  
  - **Riferimenti**: personalizzazione e strumenti interattivi (4.2 Docenti), equità (2).[^38],[^18]

### 11.5 Storia, Filosofia, Diritto ed Economia

- **Esempio 9 — Dialogo critico con “persona storica simulata”**  
  - **Obiettivo**: esercitare il **pensiero critico** distinguendo **ricostruzione plausibile** da **invenzione**.  
  - **Procedura**: l’IA simula un autore (con **disclaimer** sulla finzione); gli studenti pongono domande; il docente guida la **verifica con fonti**.  
  - **Tutela**: piattaforma **chiusa**; divieto di caricare dati personali.  
  - **Valutazione**: rubriche su **uso di fonti**, **confutazione** e **correttezza storica**.  
  - **Riferimenti**: trasparenza sull’interazione con IA (1.2; art. 50 AI Act richiamato), educazione alle fonti (4.2 Studenti).[^28],[^39]

- **Esempio 10 — Analisi di casi giuridici con explainability**  
  - **Obiettivo**: allenare **ragionamento giuridico** e **explainability** delle soluzioni.  
  - **Procedura**: l’IA propone **argomentazioni pro/contro** su un caso; gli studenti valutano **coerenza normativa** e **principi**; il docente integra.  
  - **Tutela**: casi **fittizi**; nessun dato sensibile reale.  
  - **Valutazione**: griglia su **interpretazione**, **applicazione norme**, **chiarezza argomentativa**.  
  - **Riferimenti**: diritto alla spiegazione della logica (4.3), sorveglianza umana (3.1).[^41],[^19]

### 11.6 Informatica e competenze digitali

- **Esempio 11 — Pair programming con IA come “copilota”**  
  - **Obiettivo**: sviluppare **abilità di debugging** e **comprensione del codice**.  
  - **Procedura**: l’IA propone **suggerimenti**; gli studenti devono **giustificarne** l’accettazione o il rifiuto; il docente verifica.  
  - **Tutela**: repository locale; no codice proprietario esterno.  
  - **Valutazione**: relazione su **decisioni di design** e **sicurezza**.  
  - **Riferimenti**: sistemi ibridi (4.3), sicurezza e misure tecniche (3.3).[^41],[^25]

- **Esempio 12 — Data literacy e bias**  
  - **Obiettivo**: riconoscere **bias** nei dataset (es. squilibri di classe, proxy sensibili).  
  - **Procedura**: l’IA aiuta a **profilare** un dataset sintetico; gli studenti propongono **mitigazioni** (bilanciamento, metriche di equità).  
  - **Tutela**: **dati sintetici**; nessun dato reale.  
  - **Valutazione**: rubriche su **diagnosi del bias** e **strategie di correzione**.  
  - **Riferimenti**: bias e non discriminazione (3.1), dati sintetici e PET (3.3).[^21],[^26]

### 11.7 Arte, Design, Musica

- **Esempio 13 — Moodboard generativi con citazioni corrette**  
  - **Obiettivo**: creare **moodboard** ispirazionali con **attribuzioni** e **diritti** rispettati.  
  - **Procedura**: l’IA genera varianti stilistiche; gli studenti **documentano** prompt, fonti e licenze; il docente verifica.  
  - **Tutela**: attenzione a **copyright** e **licenze**; uso di contenuti liberi.  
  - **Valutazione**: rubrica su **processo creativo**, **citazioni** e **coerenza**.  
  - **Riferimenti**: innovazione responsabile (2), trasparenza e documentazione (3.1).[^18],[^20]

- **Esempio 14 — Ear training assistito**  
  - **Obiettivo**: riconoscere **intervalli e progressioni** con feedback immediato.  
  - **Procedura**: l’IA genera sequenze; gli studenti rispondono e ricevono **feedback**; il docente personalizza i livelli.  
  - **Tutela**: profili anonimi; nessun dato sensibile.  
  - **Valutazione**: tracking dei progressi e prova pratica.  
  - **Riferimenti**: feedback immediato (4.2 Studenti), equità dei sistemi (3.2).[^39],[^23]

### 11.8 Educazione civica e cittadinanza digitale

- **Esempio 15 — “Comitato etico” sull’adozione di un chatbot scolastico**  
  - **Obiettivo**: sviluppare **consapevolezza** di **rischi/benefici** e **requisiti** (trasparenza, sorveglianza, data protection).  
  - **Procedura**: simulazione di **valutazione d’impatto**: identificazione finalità, rischi, misure; presentazione al “consiglio d’istituto” (simulato).  
  - **Tutela**: nessun dato reale; materiali fittizi.  
  - **Valutazione**: rubrica su **analisi rischi**, **mitigazioni** e **comunicazione**.  
  - **Riferimenti**: DPIA/FRIA (3.3), comunicazione e governance (5).[^25],[^42]

- **Esempio 16 — Fact‑checking assistito**  
  - **Obiettivo**: potenziare la **valutazione delle fonti** e la **verifica** delle affermazioni.  
  - **Procedura**: l’IA propone **ipotesi** e **fonti candidate**; gli studenti verificano su **fonti istituzionali**; il docente supervisiona.  
  - **Tutela**: citazioni corrette; evitare dati personali.  
  - **Valutazione**: griglia su **criteri di attendibilità** e **rigore** del fact‑checking.  
  - **Riferimenti**: DigComp (4.2 Studenti), trasparenza (3.1).[^39],[^20]

### 11.9 Inclusione e BES/DSA

- **Esempio 17 — Lettura assistita e trascrizione**  
  - **Obiettivo**: migliorare **accessibilità** mediante sintesi vocale e trascrizione automatica.  
  - **Procedura**: uso di **Immersive Reader** e **speech‑to‑text** per lezioni e materiali; personalizzazione del ritmo.  
  - **Tutela**: **no registrazioni** non autorizzate; policy d’istituto; informativa a famiglie.  
  - **Valutazione**: diari di **auto‑monitoraggio**; colloqui individuali.  
  - **Riferimenti**: privacy by design (3.3), equità (2), personalizzazione (4.2 Docenti).[^25],[^18],[^38]

- **Esempio 18 — Mappe concettuali generate e validate**  
  - **Obiettivo**: sostenere l’**organizzazione cognitiva**; rendere visibile la struttura dei concetti.  
  - **Procedura**: l’IA genera una **bozza di mappa**; gli studenti la **correggono** e **annotano**; il docente valuta coerenza.  
  - **Tutela**: nessun dato personale; salvataggio locale.  
  - **Valutazione**: rubrica su **completezza, gerarchia e relazioni**.  
  - **Riferimenti**: strumenti interattivi (4.2 Docenti), explainability (3.1).[^38],[^20]

### 11.10 Valutazione formativa e academic integrity

- **Esempio 19 — Rubriche di valutazione co‑generate**  
  - **Obiettivo**: rendere **espliciti** i criteri valutativi.  
  - **Procedura**: il docente fornisce **obiettivi** e **livelli**; l’IA suggerisce **descrittori**; il docente rifinisce e condivide con la classe.  
  - **Tutela**: nessun dato personale; repository d’istituto.  
  - **Valutazione**: coerenza tra **descrittori** e **evidenze** prodotte.  
  - **Riferimenti**: trasparenza e sorveglianza umana (3.1), focus docenti (4.2).[^20],[^38]

- **Esempio 20 — Compiti autentici “tracciabili”**  
  - **Obiettivo**: limitare la **sostituzione impropria** con strumenti generativi.  
  - **Procedura**: task che richiedono **dati locali**, **esperimenti**, **interviste**, **riflessioni personali**; l’IA è ammessa solo per **bozze**, da **documentare**.  
  - **Tutela**: anonimizzazione dei soggetti intervistati; consenso informato quando serve.  
  - **Valutazione**: rubriche che premiano **processo**, **tracce** e **citazioni**.  
  - **Riferimenti**: ruolo insostituibile del docente e responsabilità (3.1), educazione alla consapevolezza (4.4).[^19],[^46]

### 11.11 Organizzazione didattica e collaborazione

- **Esempio 21 — Pianificazione di unità didattiche e tempi d’aula**  
  - **Obiettivo**: ottimizzare la **sequenza delle attività** con suggerimenti sull’**andamento** (attivazione–scoperta–consolidamento–valutazione).  
  - **Procedura**: l’IA propone uno **scheletro di UDA**; il docente **adatta** obiettivi, attività, verifiche; collega a **PTOF**.  
  - **Tutela**: nessun dato personale; salvataggio su sistema d’istituto.  
  - **Valutazione**: confronto **ex‑post** tra piano e pratica; “lezioni apprese”.  
  - **Riferimenti**: framework in cinque fasi (4.1), valorizzazione risultati (4.1 Conclusione).[^29],[^34]

- **Esempio 22 — Co‑progettazione interdisciplinare**  
  - **Obiettivo**: favorire **interdisciplinarità** (es. Matematica–Storia: statistica delle migrazioni).  
  - **Procedura**: l’IA suggerisce **compiti di realtà** e **rubriche comuni**; i docenti validano; predisposizione **rubriche integrate**.  
  - **Tutela**: nessun dato personale; documentazione condivisa.  
  - **Valutazione**: valutazione **multidimensionale** con rubriche incrociate.  
  - **Riferimenti**: governance partecipata (4.1 Definizione–Pianificazione).[^30],[^31]

### 11.12 Preparazione ad esami e prove

- **Esempio 23 — Simulazioni guidate della prova scritta**  
  - **Obiettivo**: esercitare **strategie** di lettura e scrittura in tempi vincolati.  
  - **Procedura**: l’IA genera **varianti di tracce** simili alle prove ufficiali; il docente seleziona; **tempi e vincoli** reali.  
  - **Tutela**: nessun dato personale; archivio interno.  
  - **Valutazione**: doppia correzione (docente e auto‑valutazione con rubrica).  
  - **Riferimenti**: sistemi ad alto rischio (monitoraggio durante prove) **non** utilizzati; sorveglianza umana (3.1).[^27],[^19]

- **Esempio 24 — Orali strutturati con domande a gradiente**  
  - **Obiettivo**: allenare **argomentazione** e **padronanza lessicale**.  
  - **Procedura**: l’IA propone **batterie di domande** a difficoltà crescente; il docente **sceglie** e **adatta**; studenti preparano **schede di risposta**.  
  - **Tutela**: nessun dato personale; svolgimento in aula.  
  - **Valutazione**: rubriche per **chiarezza**, **accuratezza**, **uso di fonti**.  
  - **Riferimenti**: explainability e controllo (3.1), strumenti per studenti (4.2).[^20],[^39]

---

## Conclusioni

Le Linee guida MIM forniscono alle istituzioni scolastiche un **quadro organico** per introdurre l’IA in modo **responsabile e sostenibile**. Per le scuole secondarie di secondo grado, ciò si traduce in **scelte curricolari** e **organizzative** ponderate, in una **governance** capace di coniugare innovazione e tutela, e in una **professionalità docente** ancora più consapevole nel guidare gli studenti a **comprendere**, **valutare** e **co‑creare** con gli strumenti di IA. L’adozione **graduale**, **valutata** e **documentata**, insieme a formazione, trasparenza e inclusione, rappresenta la via maestra per trasformare le potenzialità dell’IA in **valore educativo concreto**.

---

## Appendice A — Checklist sintetica per progetti di IA a scuola

1. **Allineamento strategico**: coerenza con PTOF, Atto di indirizzo, RAV.  
2. **Caso d’uso**: finalità, benefici attesi, destinatari, impatti didattici/organizzativi.  
3. **Valutazione dei rischi**: classificazione AI Act, bias, privacy, sicurezza, impatti su diritti.  
4. **Compliance**: base giuridica; **DPIA** (e **FRIA** se alto rischio); informative; registro trattamenti.  
5. **Dati**: minimizzazione; no dati non necessari; configurazioni **no‑history**/**no‑training**; PET/dati sintetici quando possibile.  
6. **Fornitore**: certificazioni; art. 28 GDPR; sicurezza; luogo del trattamento; auditing.  
7. **Sorveglianza umana**: ruoli/ responsabilità; procedure di controllo degli output; canali di segnalazione.  
8. **Formazione**: personale e studenti; materiali di supporto; policy d’uso.  
9. **Monitoraggio**: metriche di efficacia/equità; gestione incidenti; revisione periodica.  
10. **Documentazione**: lezioni apprese; disseminazione su **Unica**; riuso e scaling.

---

## Riferimenti (rinvii ai paragrafi delle Linee guida)

[^1]: **Premessa** (pp. 3–4): contesto, fonti normative e strategiche di riferimento.  
[^2]: **1. Il ruolo dell’IA nella Scuola** (pp. 4–7): visione e potenzialità con cautele; approccio antropocentrico.  
[^3]: **1.1 Strategia e obiettivi del MIM** (pp. 4–6): finalità, pratiche vietate, responsabilità del *deployer*.  
[^4]: **Premessa** (p. 3): AI Act UE; Convenzione Consiglio d’Europa (05/09/2024); documenti della Commissione europea; Strategia nazionale 2024–2026; DDL n. 1146/2024.  
[^5]: **1.1** (pp. 5–6) e **1.2** (p. 7): classificazione rischio; obblighi deployer/fornitori; Considerando 56 AI Act.  
[^6]: **1.1** (pp. 5–6): pratiche vietate (art. 5 AI Act), incl. **emotion recognition** in contesti educativi.  
[^7]: **1.** (p. 4, nota 2): **UNESCO Recommendation on the Ethics of AI** (2021).  
[^8]: **Premessa** e **3.3** (pp. 12–18): **Ethical guidelines** della Commissione europea per educatori; EDPS 2024.  
[^9]: **1.1** (p. 5) e **Premessa** (p. 3): Strategia italiana per l’IA 2024–2026; DDL nazionale in materia di IA.  
[^10]: **3.3** (pp. 12–20): GDPR; Codice Privacy; minori come soggetti vulnerabili; principi art. 5 GDPR.  
[^11]: **5. Comunicazione e governance** (pp. 31–32): piattaforma **Unica**, mappatura progetti, dashboard, audit.  
[^12]: **1.1** (pp. 4–5): finalità strategiche (competitività, qualità, equità, consapevolezza).  
[^13]: **1.1** (p. 5): metodologia condivisa; conformità ad AI Act e protezione dati; promozione uso responsabile.  
[^14]: **1.1** (pp. 5–6): miglioramento apprendimenti; inclusione; ottimizzazione processi; qualità servizi; formazione.  
[^15]: **1.1** (pp. 5–6) e **art. 5 AI Act** richiamato: pratiche vietate e cautele per l’istruzione.  
[^16]: **1.1** (pp. 5–6) e **1.2** (p. 7): rischi e livelli (alto/limitato/minimo), obblighi; *deployer* e fornitori.  
[^17]: **1.2** (p. 7): i quattro pilastri alla base del modello di introduzione.  
[^18]: **2. Principi di riferimento** (pp. 7–9): centralità persona, equità, innovazione responsabile, sostenibilità, diritti, sicurezza.  
[^19]: **3.1 Requisiti etici** (pp. 9–11): intervento e sorveglianza umana; governance interna; ruoli.  
[^20]: **3.1** (pp. 9–10): trasparenza e explainability; allucinazioni dei LLM; documentazione e auditabilità.  
[^21]: **3.1** (p. 10): criteri anti-discriminazione; qualità dati; controllo bias.  
[^22]: **3.1** (pp. 10–11): ruoli, responsabilità; accordi con fornitori; art. 28 GDPR quando responsabili.  
[^23]: **3.2 Requisiti tecnici** (pp. 11–12): certificazioni; gestione dati; diritto di non partecipazione; equità del sistema.  
[^24]: **3.3 Requisiti normativi** (pp. 12–20): principi GDPR; DPIA; FRIA; informative; misure di sicurezza.  
[^25]: **3.3** (pp. 14–20): base giuridica; DPIA/FRIA; informative; soggetti autorizzati; responsabili; art. 32; registro; data breach.  
[^26]: **3.3** (pp. 14–16, 18–20): minori, **age‑gate**; PET; dati sintetici; trasferimenti extra SEE e garanzie.  
[^27]: **1.1** (pp. 5–6) e **Capo III AI Act** richiamato: sistemi ad alto rischio in istruzione; obblighi per *deployer*.  
[^28]: **1.2** (p. 7): rischio limitato (trasparenza) e rischio minimo (codici di condotta).  
[^29]: **4.1 Istruzioni operative** (pp. 21–25): approccio metodologico e cinque fasi.  
[^30]: **4.1 – Definizione** (pp. 22–23): analisi contesto, casi d’uso, stakeholder, PTOF; progettazione partecipata.  
[^31]: **4.1 – Pianificazione** (pp. 23–24): piano progetto, rischio, HUDERIA, comunicazione con stakeholder.  
[^32]: **4.1 – Adozione** (p. 24): approccio graduale, piano comunicazione, formazione.  
[^33]: **4.1 – Monitoraggio** (pp. 24–25): monitoraggio tecnico e gestionale, rivalutazione rischi, rendicontazione.  
[^34]: **4.1 – Conclusione** (p. 25): verifica, lezioni apprese, valorizzazione risultati, diffusione.  
[^35]: **4.2 Focus aree di applicazione** (pp. 25–28): esempi per ruoli.  
[^36]: **4.2 – Dirigente scolastico** (pp. 25–26): monitoraggio documentale, formazione, orari, comunicazione.  
[^37]: **4.2 – Personale amministrativo** (p. 26): chatbot richieste, comunicazioni, inventario.  
[^38]: **4.2 – Docenti** (pp. 26–27): personalizzazione, strumenti interattivi, rubriche, tutoraggio; riferimenti a STEM ed Educazione civica.  
[^39]: **4.2 – Studenti** (pp. 27–28): personalizzazione, accessibilità, autonomia, feedback; divieto *sentiment analysis*.  
[^40]: **4.3 Mitigazione dei rischi** (pp. 29–30): gerarchia HUDERIA, prevenzione-riduzione-ripristino.  
[^41]: **4.3** (pp. 29–30): protezione dati, manutenzione, progettazione etica, explainability, formazione, sistemi ibridi, interazione sociale, comunicazione.  
[^42]: **5. Comunicazione e governance** (pp. 31–32): Unica, mappatura, strumenti operativi, dashboard, audit.  
[^43]: **2** (pp. 7–9) e **4.2** (pp. 26–27): innovazione responsabile; integrazione con curricolo digitale e STEM.  
[^44]: **1.1** (pp. 5–6) e **3.1/3.3**: alti rischi in valutazione e monitoraggio; sorveglianza umana; DPIA/FRIA.  
[^45]: **3.3** (pp. 12–20): GDPR e adempimenti; ambienti controllati; disattivazione cronologie; registro; breach.  
[^46]: **4.1 – Adozione** (p. 24) e **4.4 Consapevolezza** (pp. 30–31): formazione di docenti e studenti; pensiero critico; alfabetizzazione.  
[^47]: **4.2 – Docenti/Studenti** (pp. 26–28): PEI/PDP; inclusione; cautele sui dati.  
[^48]: **3.2/3.3** (pp. 11–20): fornitori, art. 28 GDPR, sicurezza, trasferimenti; **4.3** manutenzione e audit.  
[^49]: **4.1–4.4** e **5** (pp. 21–32): condizioni di successo; governance; documentazione e disseminazione.
